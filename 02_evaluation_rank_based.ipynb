{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXXNCcd90JEi"
      },
      "source": [
        "# Information Retrieval and Web Analytics\n",
        "\n",
        "# Evaluation with Rank-Based Metrics\n",
        "\n",
        "\n",
        "Welcome to the second practical lab!\n",
        "\n",
        "In this session you are going to implement some common metrics used for the evaluation of an information retrieval system.\n",
        "\n",
        "\n",
        "### Unranked retrieval evaluation: Precision and Recall\n",
        "\n",
        "Having a given collection of documents:\n",
        "\n",
        "**Precision:** is the fraction of the retrieved documents (A: answer set) which is relevant to the searched query.\n",
        "\n",
        "$$\\begin{equation}\n",
        "  Precision=\\frac{{|R \\cap A|}}{{|A|}}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Recall:** is the fraction of the relevant documents (R: relevant documents set) which has been retrieved.\n",
        "\n",
        "$$\\begin{equation}\n",
        "  Recall=\\frac{{|R \\cap A|}}{{|R|}}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "![figure 2](https://drive.google.com/uc?export=view&id=1fVMVkEai_RhR2WsVg2IXAcGE0l3qwAEd)\n",
        "\n",
        "<center><caption> <u>Figure 1</u>: Precision and Recall</caption></center>\n",
        "\n",
        "\n",
        "The above definitions of Precision and Recall assumes that all documents in the set A have been examined.\n",
        "\n",
        "However, the user is not usually presented with all documents in the answer set A at once. User sees a **ranked set of documents** and examines them starting from the top. Thus, Precision and Recall vary as the user proceeds with their examination of the set A.\n",
        "\n",
        "\n",
        "If we want to benchmark different systems about how well are they ranking the result documents, we need:\n",
        "\n",
        "- A collection of **documents** that have to be representative of what we expect to find in reality;\n",
        "- A collection of **information needs** that has to be representative of what a user might have in reality;\n",
        "- Human relevance assessment for the (information need, document) pairs.\n",
        "\n",
        "Information retrieval systems (like search engines) can express the relevance for a (information need, document) pair in a **binary** way or through **multiple levels**.\n",
        "\n",
        "Here follow some common metrics used for the evaluation of a retrieval system:\n",
        "\n",
        "### Rank-Based Measures\n",
        "- ##### Binary relevance\n",
        "    - Precision at K (P@K)\n",
        "    - Average Precision at K (P@K)\n",
        "    - Mean Average Precision (MAP)\n",
        "    - Mean Reciprocal Rank (MRR)\n",
        "\n",
        "K is the number of recommendations or ranked documents\n",
        "    \n",
        "- ##### Multiple levels of relevance\n",
        "    - Normalized Discounted Cumulative Gain (NDCG)\n",
        "\n",
        "Notice that we only mentioned Precision but not Recall. Indeed, by returning all the documents for a query will result in a trivial 100% recall, Thus recall by itself is commonly not used as a metric in this context.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "TvU7TS900JEj"
      },
      "source": [
        "### Prepare Data\n",
        "We are going to test the above metrics on a ranking of results which is stored in the ```inputs/test_predictions.csv``` file. The prediction dataset contains:\n",
        "\n",
        "- **query_id**: query id.\n",
        "- **doc_id**: document id.\n",
        "- **predicted_relevance**: relevance predicted through a ranking algorithm.\n",
        "- **doc_score**: actual score of the document for the query (ground truth)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tfG0ZfH0JEj"
      },
      "source": [
        "### 1 - Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z0P0Nl30JEj"
      },
      "outputs": [],
      "source": [
        "# you may install any missing package with: \"python3 -m pip install <package_name>\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tl8uXjr0JEk"
      },
      "source": [
        "### 2 - Load data into memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3V3Tc5X0JEk"
      },
      "outputs": [],
      "source": [
        "search_results = pd.read_csv(\"/content/test_predictions.csv\")\n",
        "search_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsjSXyZT0JEk"
      },
      "source": [
        "Notice that our ground truth consists of multiple levels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7ZVwDdZ0JEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0c92f3-fe02-4593-abb9-9234655009f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ground truth of our dataset is composed of 5 Relevance Levels: [0.0, 1.0, 2.0, 3.0, 4.0]\n"
          ]
        }
      ],
      "source": [
        "print_result = search_results[\"doc_score\"].unique()\n",
        "print(\"The ground truth of our dataset is composed of {} Relevance Levels: {}\".format(len(print_result), sorted(print_result)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrBPSugD0JEl"
      },
      "source": [
        "### Binary Relevance\n",
        "\n",
        "To compute *Precision@K, Mean Average Precision* and *Mean Reciprocal Rank*, we need binary relevance (1 = relevant, 0 = not relevant).\n",
        "\n",
        "\n",
        "To simplify the task, we will consider as relevant **all documents that have actual score (y_true) equal or higher than $2$**, and not-relevant **the remaining documents**.\n",
        "\n",
        "Let's add a column `is_relevant` to our previous table `search_results` following the above rule about **relevance**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKgSMu260JEl"
      },
      "outputs": [],
      "source": [
        "search_results[\"is_relevant\"] = \"\"\"YOUR CODE HERE\"\"\"\n",
        "search_results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U79y9K8l0JEl"
      },
      "source": [
        "### 3 - Metrics\n",
        "\n",
        "#### Precision @ K (P@K)\n",
        "\n",
        "Precision at K **(P@K) measures the number of relevant results among the top K documents**. It assesses whether the users are getting relevant documents at the top of the ranking or not.\n",
        "\n",
        "A drawback of this metric is that it fails to take into account the positions of the relevant documents among the top K.\n",
        "\n",
        "Python implementation:\n",
        "Implement the function ```precision_at_k(y_true, y_score, k)``` that takes as input the true relevance labels, the predicted score, the number of documents to consider K and compute the precision as $k$.\n",
        "\n",
        "Steps:\n",
        "1. use ```np.argsort``` and [::1] to obtain the list of indexes of the predicted score sorted in descending order.\n",
        "2. use the indexes of point 1 to sort the actual relevance label of the documents (hint: ```np.take```).\n",
        "3. consider the top K relevance label of the documents (after the sorting) and retrieve the number of relevant documents (among the top K, i.e., normalise the number of relevant documents by K).\n",
        "\n",
        "Notice that the P@K is computed for a single query and the respective set of retrieved documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCXVowQ-0JEl"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(y_true, y_score, k=10):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: Ground truth (true relevance labels).\n",
        "    y_score: Predicted scores.\n",
        "    k : number of doc to consider.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    precision @k : float\n",
        "\n",
        "    '''\n",
        "    order = \"\"\"YOUR CODE HERE\"\"\"\n",
        "    y_true = \"\"\"YOUR CODE HERE\"\"\"\n",
        "    relevant = \"\"\"YOUR CODE HERE\"\"\"\n",
        "    return \"\"\"YOUR CODE HERE\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDAUKNNo0JEl"
      },
      "source": [
        "Compute Precision@10 for query with q_id=0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FshfmHO0JEl"
      },
      "outputs": [],
      "source": [
        "# Check for query 0\n",
        "\n",
        "current_query = 0\n",
        "current_query_res = search_results[search_results[\"query_id\"] == current_query]\n",
        "\n",
        "k = 5\n",
        "print(\"==> Precision@{}: {}\\n\".format(k,\n",
        "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
        "\n",
        "\n",
        "print(\"\\nCheck on the dataset sorted by score:\\n\")\n",
        "#current_query_res.sort_values(\"score\", ascending=False).head(k)\n",
        "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5naU43A0JEl"
      },
      "outputs": [],
      "source": [
        "k = 3\n",
        "print(\"==> Precision@{}: {}\\n\".format(k,\n",
        "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
        "\n",
        "\n",
        "k=10\n",
        "print(\"==> Precision@{}: {}\\n\".format(k,\n",
        "                                precision_at_k(current_query_res[\"bin_y_true\"], current_query_res[\"predicted_relevance\"], k)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2zGoKdf0JEm"
      },
      "source": [
        "#### Average Precision@K - AP@K\n",
        "\n",
        "With respect to $P@K$, $AP@K$ gives a better intuition of the model ability to sort the results for a specific query. It tells how much the relevant documents are concentrated in the highest ranked predictions.\n",
        "\n",
        "The Average Precision approximates the area under the un-interpolated precision-recall curve.\n",
        "\n",
        "$$AP@K=\\frac{1}{GTP}\\sum_k^n{P@K \\times rel@K}\\tag{1}$$\n",
        "\n",
        "where:\n",
        "- GTP is the total number of ground truth positives, within the retrieved K;\n",
        "- P@K is the Precision at K ranked results.\n",
        "- rel@K is a relevance function; it retrieves 1 if the document at rank K is relevant or 0 otherwise.\n",
        "\n",
        "![figure 1](https://drive.google.com/uc?export=view&id=1Tf8en2FW-kYvFTmvLb4O2ZpM_gMt18tY)\n",
        "\n",
        "\n",
        "<img src=\"images/apk.png\" style=\"width:600px;height:250px;\">\n",
        "<caption><center> <u> <font color=''> Figure 2 </u><font color=''>  : Computation of APK <br> (Picture taken from <i>https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52</i>)</center></caption>  \n",
        "\n",
        "(Picture taken from <i>https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52</i>)</center></caption>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58_dSThT0JEm"
      },
      "source": [
        "Python implementation:\n",
        "Implement the function ```avg_precision_at_k(y_true, y_score, k)``` that computes the average precision at K. The function takes as inputs the true relevance labels, the predicted score, and the number of documents to consider K.\n",
        "\n",
        "Notice that the Precision@K is computed for a single query and the respective set of retrieved documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-sS2-O40JEm"
      },
      "outputs": [],
      "source": [
        "def avg_precision_at_k(y_true, y_score, k=10):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: Ground truth (true relevance labels).\n",
        "    y_score: Predicted scores.\n",
        "    k : number of doc to consider.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    average precision @k : float\n",
        "    \"\"\"\n",
        "\n",
        "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
        "\n",
        "    prec_at_i = 0\n",
        "    prec_at_i_list = []\n",
        "    number_of_relevant = 0\n",
        "    number_to_iterate = min(k, len(order))\n",
        "\n",
        "    for i in range(number_to_iterate):\n",
        "        if y_true[order[i]] == 1:\n",
        "            number_of_relevant += 1\n",
        "            prec_at_i = number_of_relevant / (i + 1)\n",
        "            prec_at_i_list.append(prec_at_i)\n",
        "\n",
        "    if number_of_relevant == 0:\n",
        "        return 0\n",
        "    else:\n",
        "      return np.sum(prec_at_i_list) / number_of_relevant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSDoH26T0JEm"
      },
      "source": [
        "Compute Precision@10 for the query with q_id=0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTSeGPWR0JEm"
      },
      "outputs": [],
      "source": [
        "avg_precision_at_k(np.array(current_query_res[\"bin_y_true\"]), np.array(current_query_res[\"predicted_relevance\"]), 150)\n",
        "#avg_precision_at_k(np.array([1,0,0,1,1,0]), np.array([0.9,0.8,0.7,0.6,0.5, 0.4]),6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVKJ5uVs0JEm"
      },
      "outputs": [],
      "source": [
        "# Check with 'average_precision_score' of 'sklearn' library\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "k = 150\n",
        "temp = current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(k)\n",
        "average_precision_score(np.array(temp[\"bin_y_true\"]), np.array(temp[\"predicted_relevance\"][:k]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rnuCQuZ0JEm"
      },
      "outputs": [],
      "source": [
        "# Check with 'average_precision_score' of 'sklearn' library\n",
        "\n",
        "y_true = np.array([1, 1, 0, 1, 0, 0, 1])\n",
        "y_scores = np.array([7, 6, 5, 4, 3, 2, 1])\n",
        "assert (average_precision_score(y_true[:3], y_scores[:3]) == avg_precision_at_k(y_true, y_scores, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiiq-y_z0JEm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf4rp0aD0JEm"
      },
      "source": [
        "Manual check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5x3Nwrx0JEn"
      },
      "outputs": [],
      "source": [
        "avg_precision_at_k(np.array(current_query_res[\"bin_y_true\"]), np.array(current_query_res[\"predicted_relevance\"]), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nfUtOgx0JEn"
      },
      "outputs": [],
      "source": [
        "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_uRL03d0JEn"
      },
      "outputs": [],
      "source": [
        "(1 + (2 / 2) + (3 / 5) + (4 / 7) + (5 / 8) + (6 / 9)) / np.sum(current_query_res.sort_values(\"predicted_relevance\", ascending=False)[\"bin_y_true\"].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbzWgaDh0JEn"
      },
      "source": [
        "#### Mean Average Precision (mAP)\n",
        "\n",
        "The Mean Average Precision (mAP) is simply the **mean** of all the queries AP. This metric is not computed for a single query as previous metrics, but it takes into account all the queries.\n",
        "\n",
        "Above we mentioned that the average precision approximates the area under the un-interpolated precision-recall curve for a single query. As a consequence, the mAP is roughly the average area under the precision-recall curve for a set of queries.\n",
        "\n",
        "$$mAP=\\frac{1}{N}\\sum_{i=1}^n{AP_i}\\tag{2}$$\n",
        "\n",
        "![figure 1](https://drive.google.com/uc?export=view&id=16MkL_Hk1-7fkGeXxT3UCQLJ1MxsYDHrv)\n",
        "\n",
        "\n",
        "<img src=\"images/map.png\" style=\"width:600px;height:450px;\">\n",
        "<caption><center> <u> <font color=''> Figure 3 </u><font color=''>  : Computation of mAP </center></caption>  \n",
        "\n",
        "Implement a function ```map_at_k(search_res, k)``` that takes as input the dataset containing search results (list of actual labels, list of predicted scores, list queries) and k, and compute the Mean Average Precision (mAP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2kB_CCn0JEn"
      },
      "outputs": [],
      "source": [
        "def map_at_k(search_res, k=10):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    search_res: search results dataset containing:\n",
        "        q_id: query id.\n",
        "        doc_id: document id.\n",
        "        predicted_relevance: relevance predicted through LightGBM.\n",
        "        y_true: actual score of the document for the query (ground truth).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    mean average precision @k : float\n",
        "    '''\n",
        "    avp = []\n",
        "    for q in search_res[\"query_id\"].unique():  #loop over all query id\n",
        "        curr_data = search_res[search_res[\"query_id\"] == q]  # select data for current query\n",
        "        avp.append(avg_precision_at_k(np.array(curr_data[\"bin_y_true\"]), np.array(curr_data[\"predicted_relevance\"]),\n",
        "                                      k))  #append average precision for current query\n",
        "    return np.sum(avp) / len(avp), avp  # return mean average precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akxY6f6w0JEn"
      },
      "source": [
        "Compute mAP@10 for all queries of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRYZat0d0JEn"
      },
      "outputs": [],
      "source": [
        "map_k, avp = map_at_k(search_results, 10)\n",
        "map_k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYBFZCKy0JEn"
      },
      "source": [
        "#### Mean Reciprocal Rank (MRR)\n",
        "\n",
        "Mean Reciprocal Rank is particularly used when we are interested in 'the first' correct answer.\n",
        "\n",
        "If we define:\n",
        "\n",
        "- $R_i$ as the ranking for the query $q_i$;\n",
        "- $S_{correct}(R_i)$ as the position of the first correct answer in $R_i$\n",
        "- $K$ as the threshold for ranking position\n",
        "\n",
        "The reciprocal rank $$RR(R_i)$$ for query $q_i$ is computed as follows:\n",
        "\n",
        "$$\\begin{equation}\n",
        "  RR(R_i)==\\left\\{\n",
        "  \\begin{array}{@{}ll@{}}\n",
        "    \\frac{1}{S_{correct}(R_i)}, & \\text{if}\\ S_{correct}(R_i) $\\leq$ K \\\\\n",
        "    0, & \\text{otherwise}\n",
        "  \\end{array}\\right.\n",
        "  \\tag{3}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "\n",
        "The Mean Reciprocal Rank (MRR) can be defined as the mean of the RR for all queries:\n",
        "\n",
        "$$\\begin{equation}\n",
        "  MRR(R_i)==\\frac{1}{N}\\sum_{i=1}^N{RR(R_i)}\n",
        "\\end{equation}\n",
        "\\tag{4}\n",
        "$$\n",
        "\n",
        "where $N$ is the total number of queries (and rankings since we have a ranking per query).\n",
        "\n",
        "Implement the function ```rr_at_k(y_true, y_score, k)``` that computes the Reciprocal Rank at the threshold $k$ for a single query and then compute the MRR@K for k=3, 5 and 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHhX-L4k0JEo"
      },
      "outputs": [],
      "source": [
        "def rr_at_k(y_true, y_score, k=10):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: Ground truth (true relevance labels).\n",
        "    y_score: Predicted scores.\n",
        "    k : number of doc to consider.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Reciprocal Rank for current query\n",
        "    \"\"\"\n",
        "\n",
        "    order = \"\"\"YOUR CODE HERE\"\"\" # get the list of indexes of the predicted score sorted in descending order.\n",
        "    y_true = \"\"\"YOUR CODE HERE\"\"\" # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
        "    if \"\"\"YOUR CODE HERE\"\"\": # if there are not relevant doument return 0\n",
        "        return 0\n",
        "    return \"\"\"YOUR CODE HERE\"\"\" # hint: to get the position of the first relevant document use \"np.argmax\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg9lBYi_0JEo"
      },
      "outputs": [],
      "source": [
        "y_true = np.array([0, 1, 0, 1, 1])\n",
        "score = np.array([0.9, 0.5, 0.6, 0.7, 0.2])\n",
        "rr_at_k(y_true, score, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBORZc7K0JEo"
      },
      "source": [
        "##### Make some test with the query with q_id = 8 to check if your function is working properly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsPUEum-0JEo"
      },
      "outputs": [],
      "source": [
        "current_query = 8\n",
        "current_query_res = search_results[search_results[\"query_id\"] == current_query]\n",
        "current_query_res.sort_values(\"predicted_relevance\", ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hPYAv5U0JEo"
      },
      "outputs": [],
      "source": [
        "labels = np.array(search_results[search_results['query_id'] == 8][\"bin_y_true\"])\n",
        "scores = np.array(search_results[search_results['query_id'] == 8][\"predicted_relevance\"])\n",
        "np.round(rr_at_k(labels, scores, 10),4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1zkxaJW0JEo"
      },
      "source": [
        "##### Compute the MRR@K for k=3,5,and 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0QHj0QI0JEp"
      },
      "outputs": [],
      "source": [
        "mrr = {}\n",
        "for k in [3, 5, 10]:\n",
        "    RRs = []\n",
        "    for q in search_results['query_id'].unique():  # loop over all query ids\n",
        "        labels = np.array(search_results[search_results['query_id'] == q][\"bin_y_true\"])  # get labels for current query\n",
        "        scores = np.array(search_results[search_results['query_id'] == q][\"predicted_relevance\"])  # get predicted score for current query\n",
        "        RRs.append(rr_at_k(labels, scores, k))  # append RR for current query\n",
        "    mrr[k] = np.round(float(sum(RRs) / len(RRs)), 4)  # Mean RR at current k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_obkSIF40JEp"
      },
      "outputs": [],
      "source": [
        "mrr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ90JVxZ0JEp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYqFgmjE0JEp"
      },
      "source": [
        "### Multiple levels of relevance metrics\n",
        "\n",
        "#### NDCG - Normalized Discounted Cumulative Gain\n",
        "\n",
        "**NDCG** also works if documents relevance are a real number, i.e., when each document's relevance is not expressed in a binary form (relevant or non-relevant).\n",
        "\n",
        "This metric is especially used with machine learning based approaches, like 'Learning To Rank', and it takes values between $0$ (very poor/bad ranking) and $1$ (optimal ranking).\n",
        "\n",
        "Before defining $NDCG$, let's talk about **Discounted Cumulative Gain (DCG)**:\n",
        "**DCG** is based on the following assumptions:\n",
        "\n",
        "- Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)\n",
        "- Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents.\n",
        "\n",
        "$DCG$ is based on the notion of **Cumulative Gain (CG)**:\n",
        "**CG** does not include the position of a result in the consideration of the usefulness of a result set. It is the sum of the relevance values of all results in a search result list (in the ranking). Suppose you were presented with a set of search results for a query and asked to rank each result:\n",
        "- 0 => Not relevant\n",
        "- 1 => Near relevant\n",
        "- 2 => Relevant\n",
        "\n",
        "If we sum the values for a page of results we will have a measure of the cumulative gain (CG).\n",
        "\n",
        "$$CG = \\sum_{pos=1}^n Rel_{pos}\\tag{5}$$\n",
        "\n",
        "Where $Rel_{pos}$ is the graded relevance of $pos^{th}$ document.\n",
        "\n",
        "Cumulative gain, however, does not reward relevant results that appear higher in the result set (CG function is unaffected by changes in the ordering of search results). To achieve the Discounted Cumulative Gain (DCG) we must discount results that appear lower.\n",
        "\n",
        "**Discounted Cumulative Gain (DCG):** The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n",
        "\n",
        "$$DCG = \\sum_{pos=1}^n \\frac{Rel_{pos}}{\\log_2(pos+1)} = Rel_1 + \\sum_{pos=2}^n \\frac{Rel_{pos}}{\\log_2(pos+1)}\\tag{6}$$\n",
        "\n",
        "An alternative formulation of $DCG$ that places stronger emphasis on retrieving relevant documents is the following:\n",
        "\n",
        "$$DCG = \\sum_{pos=1}^n \\frac{2^{Rel_{pos}} -1}{\\log_2(pos+1)}\\tag{7}$$\n",
        "\n",
        "The latter formula is commonly used in industry including major web search companies. These two formulations of DCG are the same when the relevance values of documents are binary.\n",
        "\n",
        "**Normalized DCG (NDCG):** If you calculate DCG for different queries you’ll find that some queries are just harder than others and will produce lower DCG scores than easier queries. Normalization solves this problem by scaling the results based off of the best result seen. This is done by sorting all relevant documents in the corpus by their relative relevance, producing the maximum possible DCG through position $n$, also called **Ideal DCG (IDCG)** through that position (*usually it is the ground truth*).\n",
        "\n",
        "$$NDCG_{pos} = \\frac{DCG_{pos}}{iDCG}\\tag{8}$$\n",
        "\n",
        "![figure 2](https://drive.google.com/uc?export=view&id=1WaYJHzuOJTZLASaq3QWocDfapgIXuqj3)\n",
        "\n",
        "<img src=\"images/ndcg.png\" style=\"width:650px;height:450px;\">\n",
        "<caption><center> <u> <font color=''> Figure 2 </u><font color=''> : Computation of NDCG </br>\n",
        "    (Picture taken from https://medium.com/swlh/rank-aware-recsys-evaluation-metrics-5191bba16832)</center></caption>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_et8Wjg0JEp"
      },
      "source": [
        "Implement the functions:\n",
        "- ```dcg_at_k(y_score, y_true, k)``` based on formula $7$  \n",
        "- ```ndcg_at_k(y_score, y_true, k)```\n",
        "\n",
        "Compute:\n",
        "- the $NDCG@10$ for query with ```q_id=0```\n",
        "- the average $NDCG@10$ (considering all queries/rankings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNKxELzW0JEp"
      },
      "outputs": [],
      "source": [
        "def dcg_at_k(y_true, y_score, k=10):\n",
        "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
        "    y_true = np.take(y_true, order[\n",
        "                             :k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
        "    gain = 2 ** y_true - 1  # Compute gain (use formula 7 above)\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)  # Compute denominator\n",
        "    return np.sum(gain / discounts)  #return dcg@k\n",
        "\n",
        "\n",
        "def ndcg_at_k(y_true, y_score, k=10):\n",
        "    dcg_max = dcg_at_k(y_true, y_true, k)\n",
        "    if not dcg_max:\n",
        "        return 0\n",
        "    return np.round(dcg_at_k(y_true, y_score, k) / dcg_max, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTMsmlqf0JEq"
      },
      "source": [
        "##### the  𝑁𝐷𝐶𝐺@10 for query with q_id=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f9q6jpM0JEq"
      },
      "outputs": [],
      "source": [
        "q_id = 0\n",
        "k = 10\n",
        "labels = np.array(search_results[search_results['query_id'] == q_id][\"doc_score\"])\n",
        "scores = np.array(search_results[search_results['query_id'] == q_id][\"predicted_relevance\"])\n",
        "ndcg_k = np.round(ndcg_at_k(labels, scores, k), 4)\n",
        "print(\"ndcg@{} for query with query_id={}: {}\".format(k, q_id, ndcg_k))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66-3odq70JEq"
      },
      "source": [
        "##### the average  𝑁𝐷𝐶𝐺@10  (considering all queries/rankings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP8bcwf60JEq"
      },
      "outputs": [],
      "source": [
        "ndcgs = []\n",
        "k = 10\n",
        "for q in search_results['query_id'].unique():\n",
        "    labels = np.array(search_results[search_results['query_id'] == q][\"doc_score\"])\n",
        "    scores = np.array(search_results[search_results['query_id'] == q][\"predicted_relevance\"])\n",
        "    ndcgs.append(np.round(ndcg_at_k(labels, scores, k), 4))\n",
        "\n",
        "avg_ndcg = np.round(float(sum(ndcgs) / len(ndcgs)), 4)\n",
        "print(\"Average ndcg@{}: {}\".format(k, avg_ndcg))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEsOcM4i0JEq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}